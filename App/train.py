import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.cluster import KMeans
import joblib

DATA_PATH = '../Final/movies_dataset_enriched.csv'
df_raw = pd.read_csv(DATA_PATH)

# --- 1. CÃC HÃ€M Xá»¬ LÃ (GIá»® NGUYÃŠN) ---
def _parse_list_safe(x):
    if pd.isna(x) or str(x).strip() == '': return []
    if isinstance(x, str): return [i.strip() for i in x.split(',')]
    return []

def _parse_collection_to_list(x):
    if pd.isna(x) or str(x).strip() == '': return []
    return [str(x).strip()]

def time_based_target_encoding(df_sorted, list_col_name, target_col, alpha=10):
    global_mean = df_sorted[target_col].mean()
    history = {}
    feature_values = []
    
    for idx, row in df_sorted.iterrows():
        current_items = row[list_col_name]
        stats = []
        for item in current_items:
            if item in history:
                rec = history[item]
                mean_val = (rec['sum'] + alpha * global_mean) / (rec['count'] + alpha)
                stats.append(mean_val)
            else:
                stats.append(global_mean)
        
        # [TINH CHá»ˆNH] Giáº£m tá»· trá»ng Max xuá»‘ng, tÄƒng Mean lÃªn Ä‘á»ƒ á»•n Ä‘á»‹nh hÆ¡n
        if stats:
            score = 0.6 * np.max(stats) + 0.4 * np.mean(stats) 
        else:
            score = global_mean
        feature_values.append(score)

        if row[target_col] > 0:
            for item in current_items:
                if item not in history: history[item] = {'sum': 0.0, 'count': 0.0}
                history[item]['sum'] += row[target_col]
                history[item]['count'] += 1.0
    return feature_values

def prepare_features(df_input):
    df = df_input.copy()

    # --- CLEANING ---
    df['revenue_raw'] = pd.to_numeric(df['revenue'], errors='coerce').fillna(0)
    df['budget_raw'] = pd.to_numeric(df['budget'], errors='coerce').fillna(0)
    # Lá»c cháº·t hÆ¡n: Revenue > 50k
    df = df[(df['revenue_raw'] > 50000) & (df['budget_raw'] > 5000)]
    df['revenue'] = np.log1p(df['revenue_raw'])

    # --- DATE FEATURES ---
    df['release_date'] = pd.to_datetime(df.get('release_date'), errors='coerce')
    df['release_year'] = df['release_date'].dt.year
    df['release_month'] = df['release_date'].dt.month
    df['release_quarter'] = df['release_date'].dt.quarter

    # --- [Äá»˜T PHÃ 1] RELATIVE BUDGET + POLYNOMIAL ---
    # Budget tÆ°Æ¡ng Ä‘á»‘i so vá»›i nÄƒm Ä‘Ã³
    yearly_stats = df.groupby('release_year')['budget_raw'].median().reset_index().rename(columns={'budget_raw': 'year_median_budget'})
    df = df.merge(yearly_stats, on='release_year', how='left')
    df['budget_relative'] = df['budget_raw'] / (df['year_median_budget'] + 1)
    
    # Budget bÃ¬nh phÆ°Æ¡ng (MÃ´ phá»ng hiá»‡u á»©ng phi tuyáº¿n tÃ­nh: tiá»n cÃ ng nhiá»u doanh thu tÄƒng cÃ ng nhanh)
    df['budget'] = np.log1p(df['budget_raw'])
    df['budget_sq'] = df['budget'] ** 2 

    # --- LIST PARSING ---
    list_cols = ['genres', 'cast', 'production_companies', 'director', 'keywords']
    for col in list_cols:
        if col in df.columns: df[col] = df[col].apply(_parse_list_safe)
        else: df[col] = [[] for _ in range(len(df))]
    
    if 'collection' in df.columns:
        df['is_franchise'] = df['collection'].notna().astype(int)
    else: df['is_franchise'] = 0

    if 'runtime' in df.columns:
        df['runtime'] = pd.to_numeric(df['runtime'], errors='coerce').fillna(df['runtime'].median())

    # --- POSTER ---
    poster_cols = ['poster_brightness', 'poster_saturation', 'poster_dom_r', 'poster_dom_g', 'poster_dom_b']
    if all(c in df.columns for c in poster_cols):
        for c in poster_cols: df[c] = pd.to_numeric(df[c], errors='coerce').fillna(127.0)

    # --- ENCODING ---
    df = df.sort_values('release_date').reset_index(drop=True)
    df['cast_score'] = time_based_target_encoding(df, 'cast', 'revenue', alpha=5) # TÄƒng alpha lÃªn 5 Ä‘á»ƒ bá»›t nhiá»…u
    df['director_score'] = time_based_target_encoding(df, 'director', 'revenue', alpha=5)
    df['genre_score'] = time_based_target_encoding(df, 'genres', 'revenue', alpha=20)
    df['company_score'] = time_based_target_encoding(df, 'production_companies', 'revenue', alpha=10)

    # --- [Äá»˜T PHÃ 2] K-MEANS CLUSTERING (GOM NHÃ“M PHIM) ---
    # Ta sáº½ gom nhÃ³m cÃ¡c phim dá»±a trÃªn: Budget, Runtime vÃ  Year
    # Má»¥c Ä‘Ã­ch: GiÃºp model biáº¿t "Phim nÃ y thuá»™c nhÃ³m bom táº¥n dÃ i" hay "Phim ngáº¯n chi phÃ­ tháº¥p"
    print("Äang thá»±c hiá»‡n phÃ¢n cá»¥m K-Means...")
    cluster_features = df[['budget', 'runtime', 'release_year']].fillna(0)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(cluster_features)
    
    # Gom thÃ nh 8 nhÃ³m phim Ä‘iá»ƒn hÃ¬nh
    kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)
    df['movie_cluster'] = kmeans.fit_predict(scaled_features)
    
    # --- [Äá»˜T PHÃ 3] INTERACTION ---
    df['budget_x_cast'] = df['budget'] * df['cast_score']
    df['budget_x_cluster'] = df['budget'] * df['movie_cluster'] # TÆ°Æ¡ng tÃ¡c giá»¯a tiá»n vÃ  nhÃ³m phim

    # Multi-hot Genres
    mlb = MultiLabelBinarizer()
    genres_encoded = mlb.fit_transform(df['genres'])
    genres_df = pd.DataFrame(genres_encoded, columns=[f"genre_{c.replace(' ', '_')}" for c in mlb.classes_], index=df.index)
    df = df.join(genres_df)

    # Clean up
    cols_drop = ['id','title','release_date','genres','cast','production_companies','production_countries',
                 'keywords','director','original_language','rating','vote_count','popularity',
                 'collection_list','collection','temp_genre', 'revenue_raw', 'budget_raw', 'year_median_budget']
    
    df_model = df.drop(columns=[c for c in cols_drop if c in df.columns])
    
    return df, df_model.drop(columns=['revenue']), df_model['revenue'], kmeans, scaler

# --- CHáº Y QUY TRÃŒNH ---
print("ğŸš€ Äang xá»­ lÃ½ dá»¯ liá»‡u vá»›i Clustering & Polynomials...")
df_full, X, y, kmeans_model, scaler_model = prepare_features(df_raw)
print(f"Features: {X.shape[1]}")

# --- CHIáº¾N THUáº¬T SPLIT Má»šI: RANDOM SPLIT ---
# Náº¿u Project khÃ´ng báº¯t buá»™c pháº£i split theo thá»i gian (TimeSeriesSplit), 
# hÃ£y dÃ¹ng Random Split Ä‘á»ƒ kiá»ƒm tra kháº£ nÄƒng há»c thá»±c sá»± cá»§a model.
# Time-series split thÆ°á»ng cho káº¿t quáº£ tháº¥p hÆ¡n do sá»± thay Ä‘á»•i cá»§a thá»‹ trÆ°á»ng (VD: COVID).
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)

print("ğŸš€ Äang huáº¥n luyá»‡n XGBoost (Balanced Mode)...")

model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=2000,
    learning_rate=0.02,      
    max_depth=6,             
    min_child_weight=10,     # TÄƒng cao lÃªn Ä‘á»ƒ CHá»NG OVERFITTING (Quan trá»ng)
    subsample=0.8,
    colsample_bytree=0.7,
    gamma=0.5,               # TÄƒng gamma Ä‘á»ƒ cáº¯t tá»‰a nhÃ¡nh cÃ¢y thá»«a
    reg_alpha=2.0,           # TÄƒng L1 Regularization
    reg_lambda=5.0,          # TÄƒng L2 Regularization
    n_jobs=-1,
    random_state=42,
    early_stopping_rounds=50
)

model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_test, y_test)],
    verbose=200
)

y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("-" * 30)
print(f"ğŸ”¥ FINAL RMSE: {rmse:.4f}")
print(f"ğŸ”¥ FINAL R2: {r2:.4f}")

# --- CHáº Y ÄOáº N NÃ€Y CUá»I FILE TRAIN.PY Äá»‚ LÆ¯U CÃC FILE Cáº¦N THIáº¾T ---
import joblib

# 1. LÆ°u tá»« Ä‘iá»ƒn Median Budget theo nÄƒm (Äá»ƒ tÃ­nh Láº¡m phÃ¡t/Relative Budget)
# yearly_stats lÃ  biáº¿n báº¡n Ä‘Ã£ táº¡o trong hÃ m prepare_features
# Náº¿u khÃ´ng truy cáº­p Ä‘Æ°á»£c biáº¿n local, ta tÃ­nh láº¡i tá»« df_raw:
yearly_medians = df_raw.groupby(df_raw['release_date'].astype('datetime64[ns]').dt.year)['budget'].median().to_dict()
joblib.dump(yearly_medians, 'year_medians.pkl')

# 2. LÆ°u cÃ¡c tá»« Ä‘iá»ƒn Ä‘iá»ƒm sá»‘ (Cast, Director...)
def export_score_dict(df, col, target='revenue', fname='dict.pkl'):
    temp = df[[col, target]].explode(col)
    mapping = temp.groupby(col)[target].mean().to_dict()
    # Chuyá»ƒn key vá» chá»¯ thÆ°á»ng
    mapping = {str(k).lower(): v for k, v in mapping.items()}
    joblib.dump(mapping, fname)

# df_full lÃ  DataFrame sau khi Ä‘Ã£ prepare_features
export_score_dict(df_full, 'cast', fname='cast_scores.pkl')
export_score_dict(df_full, 'director', fname='director_scores.pkl')
export_score_dict(df_full, 'production_companies', fname='company_scores.pkl')
export_score_dict(df_full, 'keywords', fname='keyword_scores.pkl')

print("âœ… ÄÃ£ xuáº¥t Ä‘á»§ 5 file .pkl vÃ  3 file .joblib (model, kmeans, scaler)")